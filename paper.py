text = """
Al- though resource elasticity for stream processing appli- cations has been investigated in previous work, several challenges are not yet fully addressed [38]. As high- ighted by Tolosana-Calasanz et al. [46], mechanisms or scaling resources in cloud infrastructure can still in- cur severe delays. For stream processing engines that organise applications as operator graphs, an elastic op- eration that adds more nodes at runtime may require re- routing the data and migrating stream processing oper- ators. Moreover, as stream processing applications run for long periods of time and cannot be restarted with- out losing data, resource allocation must be performed much more carefully.  When considering solutions for managing elasticity of data streaming, this work discusses the techniques and metrics employed for monitoring the performance of data stream processing systems and the actions car- ried out during auto-scaling operations. The actions performed during auto-scaling operations include, for instance adding/removing computing resources and ad- justine the stream processing application by changing  erators, among other things.  3. Stream Processing Engines and Tools  While the first generation of SPEs were analogous to DBMSs, developed to perform long running queries over dynamic data and consisted essentially of cen- tralised solutions, the second generation introduced dis- tributed processing and revealed challenges on load bal- ancing and resource management. The third genera- tion of solutions resulted in more general application frameworks that enable the specification and execution of UDFs. This section presents a historical overview of data stream processing solutions and then discusses third-generation solutions.  3.1. Early Stream Processing Solutions  The first-generation of stream processing systems dates back to 2000s and were essentially extensions of DBMSs for performing continuous queries that, compared to today’s scenarios, did not process large amounts of data. In most systems, an application or query is a DAG whose vertices are operators that ex- ecute functions that transform one or multiple data streams and edges that define how data elements flow from one operator to another. The execution of a func- tion by an operator over an incoming data stream can result in one or multiple output streams. This section provides a select list of these systems and describes their properties.  NiagaraCQ [47] was conceived to perform two cat- egories of queries over XML datasets, namely queries that are executed as new data becomes available and continuous queries that are triggered periodically. STREAM [48] provides a Continuous Query Language (CQL) for specifying queries executed over incoming streams of structured data records. STREAM compiles CQLs queries into query plans, which comprise opera- tors that process tuples, queues that buffer tuples, and synopses that store operator state. A query plan is an operator tree or a DAG, where vertices are operators, and edges represent their composition and define how the data flows between operators. When executing a query plan, the scheduler selects plan operators and as- signs them to available resources. Operator scheduling presents several challenges as it needs to respect con- straints concerning query response time and memory utilisation. STREAM uses a chain scheduling technique that aims to minimise memory usage and adapt its exe- generated by monitoring applications. Similar to STREAM, it enables continuous queries that are viewed as DAGs whose vertices are operators, and edges that define the tuple flow between operators. Aurora sched- ules operators using a technique termed as train schedul- ing that explores non-linearities when processing tuples by essentially storing tuples at the input of so-called boxes, thus forming a train, and processing them in batches. It pushes tuple trains through multiple boxes hence reducing I/O operations.  As a second-generation of stream processing sys- tems, Medusa [51] uses Aurora as its query processing engine and arranges its queries to be distributed across nodes, routeing tuples and results as needed. By en- abling distributed processing and task migration across participating nodes, Medusa introduced several chal- lenges in load balancing, distribute load shedding [52], and resource management. For instance, the algorithm or selecting tasks to offload must consider the data flow among operators. Medusa offers techniques for balanc- ing the load among nodes, including a contract-based scheme that provides an economy-inspired mechanism or overloaded nodes to shed tasks to other nodes. Bo- realis [53] further extends the query functionalities of Aurora and the processing capabilities of Medusa [51] by dynamically revising query results, enabling query modification, and distributing the processing of opera- tors across multiple sites. Medusa and Borealis have been key to distributed stream processing, even though their operators did not allow for the execution of user- defined functions, a key feature of current stream pro- cessing solutions.  3.2. Current Stream Processing Solutions  Current systems enable the processing of unbounded data streams across multiple hosts and the execution of UDFs. Numerous frameworks have been proposed for distributed processing following essentially two models (Figure 6):  e the operator-graph model described earlier, where a processing system is continuously ingesting data that is processed at a by-tuple level by a DAG of operators; and  e a micro-batch in which incoming data is grouped during short intervals, thus triggering a batch pro- cessing towards the end of a time window. The rest  of this section provides a description of select sys- teme that fall into thece twa cateonries  icro-batch execution  Figure 6: Streaming processing approaches.  3.2.1. Apache Storm  An application in Storm, also called a Topology, is a computation graph that defines the processing elements (i.e. Spouts and Bolts) and how the data (i.e. tuples) flows between them. A topology runs indefinitely, or until a user stops it. Similarly to other application mod- els, a topology receives an influx of data and divides it into chunks that are processed by tasks assigned to clus- ter nodes. The data that nodes send to one another is in the form of sequences of Tuples, which are ordered lists of values.  Worker node Worker node  Master node  FF if Worker ){ Worker )/|{ Worker ){ Worker Nimbus || process || process process || process \7 AL JS  [ Supervisor |  ( Supervisor  Worker node  | Worker node | |  f Supervisor | ( Supervisor  ( Worker process  Worker process  Worker process  Worker process  TT  JvM  Executor Executor  mo |} Spout Spout || Spout or or or  Bolt) | | Bolt _)\_ Bolt _)  Figure 7: Main components of a Storm cluster [16].  Figure 7 depicts the main components of a Storm cluster [16]. Storm uses a master-slave execution ar- chitecture where a Master Node, which runs a dae- mon called Nimbus, is responsible for scheduling tasks among Worker Nodes and for maintaining a member- ship list to ensure reliable data processing. Nimbus in- teracts with Zookeeper [54] to detect node failure and reassign tasks accordingly if needed. A Storm cluster comprises multiple worker nodes, each worker repre- senting a virtual or physical machine. A worker node runs a Supervisor daemon, and one or multiple Worker Processes, which are processes (i.e. a JVM) spawned by Storm and able to run one or more Executors. An executor thread executes one or more tasks. A Task is both a realisation of a topology node and an abstraction external source and generating the data influx processed by the topology nodes. A Bolt listens to data, accepts a tuple, performs a computation or transformation — e.g. filtering, aggregation, joins, query databases, and other UDFs — and optionally emits a new tuple.  Storm has many configuration options to define how topologies make use of host resources. An administra- tor can specify the number of worker processes that a node can create, also termed slots, as well as the amount of memory that slots can use. To parallelise nodes of a Storm topology a user needs to provide hints on how many concurrent tasks each topology component should run or how many executors to use; the latter influences how many threads will execute spouts and bolts. Tasks resulting from parallel Bolts perform the same function over different sets of data but may execute in differ- ent machines and receive data from different sources. Storm’s scheduler, which is run by the Master, assigns tasks to workers in a round-robin fashion.  Storm allows for new worker nodes to be added to an existing cluster on which new topologies and tasks can be launched. It is also possible to modify the number of worker processes and executors spawned by each pro- cess. Modifying the level of parallelism by increasing or reducing the number of tasks that a running topol- ogy can create or the number of executors that it can use is more complex and, by default, requires the topol- ogy to be stopped and rebalanced. Such operation is ex- pensive and can incur a considerable downtime. More- over, some tasks may maintain state, perform grouping or hashing of tuple values that are henceforth assigned to specific downstream tasks. Stateful tasks complicate the dynamic adjustment of a running topology even fur- ther. As described in Section 5, existing work has at- tempted to circumvent some of these limitations to en- able resource elasticity.  Further performance tuning is possible by adjusting the length of executors’ input and output queues, and worker processes’ queues; factors that can impact the behaviour of the framework and its performance. Ex- isting work has proposed changes to Storm to provide more predictable performance and hence meet some of the requirements of real time applications [55]. By us- ing Trident, Storm can also perform micro-batch pro- cessing. Trident topologies can be designed to act on batches of tuples that are grouped during short intervals and then processed by a task topology. Storm is also used by frameworks that provide high-level program- ming abstractions such as Summingbird [8] that mix  While maintaining API compatibility with Apache Storm, Twitter’s Heron [35] was built with a range of architectural improvements and mechanisms to achieve better efficiency and to address several of Storm issues highlighted in previous work [56]. Heron topologies are process-based with each process running in isolation, which eases debugging, profiling, and troubleshooting. By using its built-in back pressure mechanisms, topolo- gies can self-adjust when certain components lag.  Similarly to Storm, Heron topologies are directed graphs whose vertices are either Spouts or Bolts and edges represent streams of tuples. The data model con- sists of a logical plan, which is the description of the topology itself and is analogous to a database query; and the physical plan that maps the actual execution logic of a topology to the physical infrastructure, including the machines that run each spout or bolt. When consid- ering the execution model, Heron topologies comprise the following main components: Topology Master, Con- tainer, Stream Manager, Heron Instance, Metrics Man- ager, and Heron Tracker.  Heron) Container  Instance | Heron [ Instance  eeecrs Heron | | Manager  Instance | >  Stream <— Manager  Topology Master  I plan, / physical plan and  execution state  £ 2 ES > a 2 Zookeeper 2 J Heron) Container} | § if Instance 5 Topology — = Master Stream Heron (Standby)| — sync| Manager Instance [+ Met . \ J { Metrics one Heron | manasa}  Instance |  ——  Stream (Gateway da )gatarout It E: aN M - xecution anager Thread / “(eit  ——~ tnetrics-out L thetrics-out J  Manager  [ Metrics  Figure 8: Main architecture components of a Heron topology [35].  Heron provides a command-line tool for submitting topologies to the Aurora Scheduler, a scheduler built to run atop Mesos [57]. Heron can also work with other schedulers including YARN, and Amazon EC2 Con- tainer Service (ECS) [58]. Support to other schedulers is enabled by an abstraction designed to avoid the com- plexity of Storm Nimbus, often highlighted as an archi- tecture issue in Storm. A topology in Heron runs as an Aurora job that comprises multiple Containers.  When a topology is deploved. Heron starts a single ure 8). The TM manages the topology throughout its entire life cycle until a user deactivates it. Zookeeper [54] is used to guarantee that there is a single TM for the topology and that it is discoverable by other pro- cesses. The TM also builds the physical plan and serves as a gateway for topology metrics. Heron allows for creating a StandBy TM in case the main TM fails. Con- tainers communicate with the TM hence forming a fully connected graph. Each container hosts multiple Heron Instances (HIs), a Stream Manager (SM), and a Metrics Manager (MM). An SM manages the routing of tuples, whereas SMs in a topology form a fully connected net- work. Each HI communicates with its local SM when sending and receiving tuples. The work for a spout and a bolt is carried out by HIs, which unlike Storm work- ers, are JVM processes. An MM gathers performance metrics from components in a container, which are in turn routed both to the TM and external collectors. An Heron Tracker (HT) is a gateway for cluster-wide infor- mation about topologies.  An HI follows a two-threaded design with one thread responsible for executing the logic programmed as a spout or bolt (i.e. Execution), and another thread for communicating with other components and carrying out data movement in and out of the HI (i.e. Gateway). The two threads communicate with one another via three unidirectional queues, of which two are used by the Gateway to send/receive tuples to/from the Execution thread, and another is employed by the Execution thread to export collected performance metrics.  3.2.3. Apache S4  The Simple Scalable Streaming System (S4) [59] is a distributed stream processing engine that uses the ac- tor model for managing concurrency. Processing Ele- ments (PEs) perform computation and exchange events, where each PE can handle data events and either emit new events or publish results.  S4 can use commodity cluster hardware and em- ploys a decentralised and symmetric runtime architec- ture comprising Processing Nodes (PNs) that are homo- geneous concerning functionality. As depicted in Figure 9, a PN is a machine that hosts a container of PEs that receive events, execute user-specified functions over the data, and use the communication layer to dispatch and emit new events. ZooKeeper [54] provides features used for coordination between PNs.  When developing a PE, a developer must specify its functionality and the type of events it can consume. While most PEs can only handle events with given keyed attribute values, S4 provides a keyless PE used by its input layer to handle all events that it receives.  Processing Node  Processing Element Container  Per || PE2 | eee | Pen | t i Event . F 7 Listener Dispatcerl Emiter 7 ty  Communication Layer  Zookeeper  Figure 9: A processing node in S4 [59].  PNs route events using a hash function of their keyed attribute values. Following receipt of an event, a lis- tener passes it to the processing element container that in turn delivers it to the appropriate PEs.  3.2.4. Apache Samza  Apache Samza [60] is a stream processing frame- work that uses Apache Kafka for messaging and Apache YARN [61] for deployment, resource management, and security. A Samza application is a data flow that consists of consumers that fetch data events that processed by a graph of jobs, each job containing one or multiple tasks. Unlike Storm, however, where topologies need to be de- ployed as a whole, Samza does not natively support the DAG topologies. In Samza, each job is an entity that can be deployed, started or stopped independently.  Like Heron, Samza uses single-threaded processes (containers), mapped to one CPU core. Each Samza task contains an embedded key-value store used to record state. Changes to this key-value store are repli- cated to other machines in the cluster allowing for tasks to be restored quickly in case of failure.  3.2.5. Apache Flink  Flink offers a common runtime for data streaming and batch processing applications [62]. Applications are structured as arbitrary DAGs, where special cycles are enabled via iteration constructs. Flink works with the notion of streams onto which transformations are performed. A stream is an intermediate result, whereas a transformation is an operation that takes one or more streams as input, and computes one or multiple streams. During execution, a Flink application is mapped to a streaming workflow that starts with one or more sources, comprises transformation operators, and ends with one or multiple sinks. Although there is often a mapping of one transformation to one dataflow operator, under cer- tain cases, a transformation can result in multiple oper- ators. Flink also provides APIs for iterative graph pro- cessing, such as Gelly [63].  The parallelism of Flink applications is determined by the degree of parallelism of streams and individual operators. Streams can be divided into stream partitions whereas operators are split into subtasks. Operator sub- tasks are executed independently from one another in different threads that may be allocated to different con- tainers or machines.  Worker  Task jot  Task Slot  Task Task  Slot  Task Slot  Task Slot  Slot  : { Memory & I/O Manager }  Thread Memory & I/O Manager J!  Network Manager /(|_Network Manager]!  ‘Actor System  Deploy/stop/ Flink Application cancel tasks  Program datafio  Actor System i Scheduler  dataflow)  Figure 10: Apache Flink’s execution model [62].  Flink’s execution model (Figure 10) comprises two types of processes, namely a master also called the Job- Manager and workers termed as TaskManagers. The JobManager is responsible for coordinating the schedul- ing tasks, checkpoints, failure recovery, among other functions. TaskManagers execute subtasks of a Flink dataflow. They also buffer and exchange data streams. A user can submit an application using the Flink client, which prepares and sends the dataflow to a JobManager.  Similar to Storm, a Flink worker is a JVM pro- cess that can execute one or more subtasks in separate threads. The worker also uses the concept of slots to configure how many execution threads can be created. Unlike Storm, Flink implements its memory manage- ment mechanism that enables a fair share of memory that is dedicated to each slot.  3.2.6. Spark Streaming  Apache Spark is a cluster computing solution that extends the MapReduce model to support other types of computations such as interactive queries and stream processing [64]. Designed to cover a variety of work- loads, Spark introduces an abstraction called Resilient  10  Distributed Datasets (RDDs) that enables running com- putations in memory in a fault-tolerant manner. RDDs, which are immutable and partitioned collections of records, provide a programming interface for perform- ing operations, such as map, filter and join, over mul- tiple data items. For fault-tolerance purposes, Spark records all transformations carried out to build a dataset, thus forming the so-called lineage graph.  t=1: Input Batch operation Soa ——  Immutable _ [ ) ORS { Immutable dataset = =) on = dataset t=2: ™S a a  CL) CL  Seee a, : rr) D-Stream 1 D-Stream 2  Figure 11: D-Stream processing model [65].  Under the traditional stream processing approach based on a graph of continuous operators that process tuples as they arrive, it is arguably difficult to achieve fault tolerance and handle stragglers. As application state is often kept by multiple operators, fault tolerance is achieved either by replicating sections of the process- ing graph or via upstream backup. The former demands synchronisation of operators via a protocol such as Flux [66] or other transactional protocols [67], whereas the latter, when a node fails, requires parents to replay pre- viously sent messages to rebuild the state.  To handle faults and stragglers more efficiently, Za- haria et al. [65] proposed D-Streams, a discretised stream processing based on Spark Streaming. As de- picted in Figure 11, D-Streams follows a micro-batch approach that organises stream processing as batch computations carried out periodically over small time windows. During a short time interval, D-Streams stores the received data, which the cluster resources then use as input dataset for performing parallel computa- tions once the interval elapses. These computations pro- duce new datasets that represent an intermediate state or computation outputs. The intermediate state consists of RDDs that D-Streams processes along with the datasets stored during the next interval. In addition to providing a strong unification with batch processing, this model stores the state in memory as RDDs [64] that D-Streams can deterministically recompute. System S, a precursor to IBM Streams’, is a mid- dleware that organises applications as DAGs of oper- ators and that supports distributed processing of both structured and unstructured data streams. Stream Pro- cessing Language (SPL) offers a language and engine for composing distributed and parallel data-flow graphs and a toolkit for building generic operators [44]. It pro- vides language constructs and compiler optimisations that utilise the performance of the Stream Processing Core (SPC) [68]. SPC is a system for designing and de- ploying stream processing DAGs that support both re- lational operators and user-defined operators. It places operators on containers that consist of processes run- ning on cluster nodes. The SPC data fabric provides the communication substrate implemented on top of a col- lection of distributed servers.  Esc [69] is another stream processing engine that also follows the data-flow scheme where programs are DAGs whose vertices represent operations performed on the received data and edges are the composition of opera- tors. The Esc system, which uses the actor model for concurrency, comprises a system and multiple machine processes responsible for executing workers.  Other systems, such as TimeStream [70], use a DAG abstraction for structuring an application as a graph of operators that execute user-defined functions. Employ- ing a graph abstraction is not exclusive to data stream processing. Other big data processing frameworks [71] also provide high-level APIs that enable developers to specify computations as a DAG. The deployment of such computations is performed by engines using re- source management systems such as Apache YARN.  Google’s MillWheel [72] also employs a data flow abstraction in which users specify a graph of transfor- mations, or computations, that are performed on input data to produce output data. MillWheel applications run on a dynamic set of hosts where each computation can run on one or more machines. A master node manages load distribution and balancing by dividing each compu- tation into a set of key intervals. Resource utilisation is continuously measured to determine increased pressure, in which case intervals are moved, split, or merged.  The Efficient, Lightweight, Flexible (ELF) stream processing system [73] uses a decentralised architecture with ‘in-situ’ data access where each job extracts data directly from a Web server, placing it in compressed  TBM has rebranded its data stream processing solution a few times over the years. Although some papers mention System S and  Trnfn Gehan Chenmeme hacenftes ere oennlac, eleede: TOM Chun mec te wa  The data is subsequently aggregated using shared re- ducer trees mapped to a set of worker processes ex- ecuted by agents structured as an overlay built using Pastry Distributed Hash Table (DHT). ELF attempts to overcome some of the limitations of existing solutions that require data movement across machines and where the data must be somewhat stale before it arrives at the stream processing system.  4. Managed Cloud Systems  This section describes public cloud solutions for pro- cessing streaming data and presents details on how elas- ticity features are made available to developers and end users. The section primarily identifies prominent tech- nological solutions for processing of streaming data and highlights their main features.  4.1. Amazon Web Services (AWS) Kinesis  A streaming data service can use Firehose for de- livering data to AWS services such as Amazon Red- shift, Amazon Simple Storage Service (S3), or Amazon Elasticsearch Service (ES). It works with data produc- ers or agents that send data to Firehose, which in turn delivers the data to the user-specified destination or ser- vice. When choosing S3 as the destination, Firehose copies the data to an $3 bucket. Under Redshift, Fire- hose first copies the data to an S3 bucket before noti- fying Redshift. Firehose can also deliver the streaming data to an ES cluster.  Firehose works with the notion of delivery streams to which data producers or agents can send data records of up to 1000 KB in size. Firehose buffers incoming data up to a buffer size or for a given buffer interval in seconds before it delivers the data to the destination service. Integration with the Amazon CloudWatch [74] enables monitoring the number of bytes transferred, the number of records, the success rate of operations, time taken to perform certain operations on delivery streams, among others. AWS enforces certain limits on the rate of bytes, records and number of operations per delivery stream, as well as streams per region and AWS account.  Amazon Kinesis Streams is a service that enables continuous data intake and processing for several types of applications such as data analytics and reporting, infrastructure log processing, and complex event pro- cessing. Under Kinesis Streams producers continu- ously push data to Streams, which is then processed by consumers. A stream is an ordered sequence of on Amazon Elastic Compute Cloud (EC2). A shard has a fixed data capacity regarding reading operations and the amount of data read per second. The total capacity of a stream is the aggregate capacity of all of its shards. Integration with Amazon CloudWatch allows for moni- toring the performance of the available streams. A user can adjust the capacity of a stream by resharding it. Two operations are allowed for respectively increasing or de- creasing available capacity, namely splitting an existing shard or merging two shards.  4.2. Google Dataflow  Google Cloud Dataflow [9] is a programming model and managed service for developing and executing a va- riety of data processing patterns such as Extract, Trans- form, and Load (ETL) tasks, batch processing, and con- tinuous computing.  Dataflow’s programming model enables a developer to specify a data processing job that is executed by the Cloud Dataflow runner service. A data processing job is specified as a Pipeline that consists of a directed graph of steps or Transforms. A transform takes one or more PCollection’s — that represent data sets in the pipeline —as input, performs the user-provided processing func- tion on the elements of the PCollection and produces an output PCollection. A PCollection can hold data of a fixed size, or an unbounded data set from a continuously updating source. For unbounded sources, Dataflow en- ables the concept of Windowing where elements of the PCollection are grouped according to their timestamps. A Trigger can be specified to determine when to emit the aggregate results of each window. Data can be loaded into a Pipeline from various //O Sources by using the Dataflow SDKs as well as written to output Sinks us- ing the sink APIs. As of writing, the Dataflow SDKs are being open sourced under the Apache Beam incubator project [75].  The Cloud Dataflow managed service can be used to deploy and execute a pipeline. During deployment, the managed service creates an execution graph, and once deployed the pipeline becomes a Dataflow job. The Dataflow service manages services such as Google Compute Engine [76] and Google Cloud Storage [77] to run a job, allocating and releasing the necessary re- sources. The performance and execution details of the job are made available via the Monitoring Interface or using a command-line tool. The Dataflow service at- tempts to perform certain automatic job optimisations such as data partitioning and parallelisation of worker code. optimisations of averegation operations or fusing  partitioning are also possible via Autoscaling and Dy- namic Work Rebalancing. For bounded data in batch mode Dataflow chooses the number of VMs based on both the amount of work in each step of a pipeline and the current throughput. Although autoscaling can be used by any batch pipeline, as of writing autoscaling for streaming-mode is experimental and participation is restricted to invited developers. It is possible, however, to adjust the number of workers assigned to a streaming pipeline manually, which replaces a running job with a new job while preserving the state information.  4.3. Azure Stream Analytics  Azure Stream Analytics (ASA) enables real-time analysis of streaming data from several sources such as devices, sensors, websites, social media, applications, infrastructures, among other sources [78].  A job definition in ASA comprises data inputs, a query, and data output. Input is the data streaming source from which the job reads the data, a query trans- forms the received data, and the output is to where the job sends results. Stream Analytics provides integration with multiple services and can ingest streaming data from Azure Event Hubs and Azure IoT Hub, and his- torical data from Azure Blob service. It performs an- alytic computations that are specified in a declarative language; a T-SQL variant termed as Stream Analytics Query Language. Results from Stream Analytics can be written to several data sinks such as Azure Storage Blobs or Tables, Azure SQL DB, Event Hubs, Azure Service Queues, among other sinks. They can also be visualised or further processed using other tools de- ployed on Azure compute cloud. As of writing, Stream Analytics does not support UDFs for data transforma- tion.  The allocation of processing power and resource ca- pacity to a Stream Analytics job is performed consid- ering Streaming Units (SUs) where an SU represents a blend of CPU capacity, memory, and read/write data rates. Certain query steps can be partitioned, and some SUs can be allocated to process data from each parti- tion, hence increasing throughput. To enable partition- ing the input data source must be partitioned and the query modified to read from a partitioned data source.  5. Elasticity in Stream Processing Systems  Over time several types of applications have benefited from resource elasticity, a key feature of cloud comput- ine [79]. As highlighted by Lorido-Botran et al.. elas- Monitoring, Analysis, Planning and Execution (MAPE) process where:  1. application and system metrics are monitored;  2. the gathered information is analysed to assess cur- rent performance and utilisation, and optionally predict future load;  3. based on an auto-scaling policy an auto-scaler cre- ates an elasticity plan on how to add or remove capacity; and  4. the plan is finally executed.  After analysing performance data, an auto-scaler may choose to adjust the number of resources (e.g. add or re- move compute resources) available to running, newly submitted, applications. Managing elasticity of data stream processing applications often requires solving two inter-related problems: (i) allocating or releasing IT resources to match an application workload; and (ii) devising and performing actions to adjust the ap- plication to make use of the additional capacity or re- lease previously allocated resources. The first problem, which consists in modifying the resource pool available for a stream processing application, is termed here as elastic resource management. A decision made by a resource manager to add/remove resource capacity for a stream processing application is referred to as scale out/in plan®. We refer to the actions taken to adjust an application during a scale out/in plan as elasticity ac- tions.  Similarly to other services running in the cloud, elas- tic resource management for data stream processing applications can make use of two types of elasticity, namely vertical and horizontal (Figure 12), which have their impact on the kind of elastic actions for adapting an application. Vertical elasticity consists in allocating more resources such as CPU, memory and network ca- pacity on a host that has previously been allocated to a given application. As described later, stream processing can benefit from this type of elasticity by, for instance, increasing the instances of a given operator (i.e. oper- ator fission [80]). Horizontal elasticity consists essen- tially in allocating additional computing nodes to host a running application.  To make use of additional resources and improve ap- plication performance, auto-scaling operations may re- quire adjusting applications dynamically by, for exam- ple, performing optimisations in their execution graphs,  3The term scale out/in is often employed in horizontal elasticity, but a plan can also be scale up/down when using vertical elasticity. For brevity, we use only scale out/in in the rest of the text  13  Elasticity type Vertical  Horizontal  Figure 12: Types of elasticity used by elastic resource management.  or modifying intra-query parallelism by increasing the number of instances of certain operators. Previous work has discussed approaches on reconfiguration schemes to modify the placement of stream processing operators dynamically to adjust an application to current resource conditions or provide fault-tolerance [81]. The litera- ture on data stream processing often employs the term elastic to convey operator placement schemes that en- able applications to deliver steady performance as their workload increases, not necessarily exploring the types of elasticity mentioned above.  Although the execution of scale out/in plans presents similarities with other application scenarios (e.g. adding/removing resources from a resource pool), ad- justing a stream processing system and applications dy- namically to make use of the newly available capacity or release unused resources is not a trivial task. The enforcement of scale out/in plans faces multiple chal- lenges. Horizontal elasticity often requires adapting the graph of processing elements and protocols, export- ing and saving operator state for replication, fault tol- erance and migration. As highlighted by Sattler and Beier [38], performing parallel processing is often dif- ficult in the case of window- or sequence-based oper- ators including CEP operators due to the amount of state they keep. Elastic operations, such as adding nodes or removing unused capacity, may require at least re-routing the data, changing the manner an incoming dataflow is split among parallel processing elements, among other issues. Such adjustments are costly to per- form, particularly if processing elements maintain state. As stream processing queries are often treated as long running that cannot be restarted without incurring a loss of data, the initial operator placement (also called task assignment), where processing elements are deployed on available computing resources becomes more criti- cal than in other systems.  Elasticity actions Static  . Resource pool Online reconfiguration Application reconfiguration  Figure 13: Elasticity actions for stream processing engines.  Given how important the initial task assignment is to guarantee the elasticity of stream processing systems, namely static and online as depicted in Figure 13. When considering the operator DAG based solutions, static techniques comprise optimisations made to modify the original graph (i.e. the logical plan) to improve task par- allelism and operator placement, optimise data trans- fers, among other goals [80]. Previous work provided a survey of various static techniques [81]. Online ap- proaches comprise both actions to modify the pool of available resources and dynamic optimisations carried out to adjust applications dynamically to utilise newly allocated resources. The next sections provide more details on how existing solutions address challenges in these categories with a focus on online techniques.  5.1. Static Techniques  A review of strategies for placing processing oper- ators in early distributed data stream processing sys- tems has been presented in previous work [81]. Sev- eral approaches for optimising the initial task assign- ment or scheduling exploit intra-query parallelism by ensuring that certain operators can scale horizontally to support larger numbers of incoming tuples, thus achiev- ing greater throughput.  R-Storm [82] handles the problem of task assign- ment in Apache Storm by providing custom resource- aware scheduling schemes. Under the considered ap- proach, each task in a Storm topology has soft CPU and bandwidth requirements and a hard memory require- ment. The available cluster nodes, on the other hand, have budgets for CPU, bandwidth and memory. While considering the throughput contribution of a data sink, given by the rate of tuples it is processing, R-Storm aims to assign tasks to a set of nodes that increases overall throughput, maximises resource utilisation, and respects resource budgets. The assignment scenario results is a quadratic multiple 3-dimensional knapsack problem. After reviewing existing solutions with several variants of knapsack problems, the authors concluded that ex- isting methods are computationally expensive for dis- tributed stream processing scenarios. They proposed scheduling algorithms that view a task as a vector of resource requirements and nodes as vectors of resource budgets. The algorithm uses the Euclidean distance be- tween a task vector and node vectors to select a node to execute a task. It also uses heuristics that attempt to place tasks that communicate in proximity to one an- other, that respect hard constraints, and that minimise resource waste.  Pietzuch et al. [83] create a Stream-Based Over- lav Network (SBON) between a stream processing en-  tency. The system architecture uses an adaptive opti- misation technique that creates a multidimensional Eu- clidean space, termed as the cost space, over which the placement is projected. Optimisation techniques such as spring relaxation are used to compute operator placement using this mathematical space. A proposed scheme maps a solution obtained using the cost space onto physical nodes.  The scheme proposed by Zhou et al. also [84] for the initial operator placement attempts to minimise the communication cost whereas the dynamic approach considers load balancing of scheduled tasks among available resources. The initial placement schemes group operators of a query tree into query fragments and try to minimise the number of compute nodes to which they are assigned. Ahmad and Cetintemel [85] also pro- posed algorithms for the initial placement of operators while minimising the bandwidth utilised in the network, even though it is assumed that the algorithms could be applied periodically.  5.2. Online Techniques  Systems for providing elastic stream processing on the cloud generally comprise two key elements:  e asubsystem that monitors how the stream process- ing system is utilising the available resources (e.g. use of CPU, memory and network resources) [86] and/or other service-level metrics (e.g. number of tuples processed over time, tail end-to-end latency [87], critical paths [88]) and tries to identify bottle- neck operators; and  e a scaling policy that determines when scale out/in plans should be performed [89].  As mentioned earlier, in addition to adding/removing resources, a scale out/in plan is backed by mechanisms to adjust the query graph to make efficient use of the up- dated resource pool. Proposed mechanisms consist of, for instance, increasing operator parallelism; rewriting the query graph based on certain patterns that are em- pirically proven to improve performance and rewriting tules specified by the end user; and migrating operators to less utilised resources.  Most solutions are application and workload agnos- tic — i.e. do not attempt to model application behaviour or detect changes in the incoming workload [90] — and offer methods to: (i) optimise the initial scheduling, when processing tasks are assigned to and deployed onto available resources: and/or (ii) reschedule process- resource pool. Operators are treated as black boxes and (re)scheduling and elastic decisions are often taken con- sidering a performance metric. Certain solutions that are not application-agnostic attempt to identify work- load busts and behaviours by considering characteristics of the incoming data as briefly described in Section 5.3.  Sattler and Beier [38] argue that distributing query nodes or operators can improve reliability “by intro- ducing redundancy, and increasing performance and/or scalability by load distribution”. They identify opera- tor patterns — e.g. simple standby, check-pointing, hot standby, stream partitioning and pipelining — for build- ing rules for restructuring the physical plan of an ap- plication graph, which can increase fault tolerance and achieve elasticity. They advocate that re-writings should be performed when a task becomes a bottleneck; i.e. it cannot keep up with the rate of incoming tuples. An existing method is used to scan the execution graph and find critical paths based on monitoring information gathered during query execution [88].  While dynamically adjusting queries with stateless operators can be difficult, modifying a graph of state- ful operators to increase intra-query parallelism is more complex. As stated by Fernandez et al. [86], during ad- justment, operator “state must be partitioned correctly across a larger set of VMs”. Fernandez et al. hence pro- pose a solution to manage operator state, which they in- tegrate into a stream processing engine to provide scale out features. The solution offers primitives to export operator state as a set of tuples, which is periodically check-pointed by the processing system. An operator keeps state regarding its processing, buffer contents, and routeing table. During a scale out operation, the key space of the tuples that an operator handles is reparti- tioned, and its processing state is split across the new operators. The system measures CPU utilisation peri- odically to detect bottleneck operators. If multiple mea- surements are above a given threshold, then the scale- out coordinator increases the operator parallelism.  Previous work has also attempted to improve the as- signment of tasks and executors to available resources in Storm and to reassign them dynamically at runtime according to resource usage conditions. T-Storm [91] (i.e. Traffic-aware Storm), for instance, aims to reduce inter-process and inter-node communication, which is shown to degrade performance under certain workloads. T-Storm monitors workload and traffic load during run- time. It provides a scheduler that generates a task sched- ule periodically, and a custom Storm scheduler that fetches the schedule and executes it by assigning ex- ecutors accordingly. Aniello et al. provide a similar approach, with two custom Storm schedulers, one for  15  offline static task assignment and another for dynamic scheduling [92]. Performance monitoring components are also introduced, and the proposed schedulers aim to reduce inter-process and inter-node communication.  Lohrmann et al. [89] introduced policies that use ap- plication or system performance metrics such as CPU utilisation thresholds, the rate of tuples processed per operator, and tail end-to-end latency. They propose a strategy to provide latency guarantees in stream pro- cessing systems that execute heady UDF data flows while aiming to minimise resource utilisation. The re- active strategy (i.e. ScaleReactively) aims to enforce la- tency requirements under varying load conditions with- out permanently overprovisioning resource capacity. The proposed solution assumes homogeneous cluster nodes, effective load balancing of elements executing UDFs, and elastically scalable UDFs. The system archi- tecture comprises elements for monitoring the latency incurred by operators in a job sequence. The reactive strategy uses two techniques, namely Rebalance and ResolveBottlenecks. The former adjusts the parallelism of bottleneck operators whereas the latter, as the name implies, resolves bottlenecks by scaling out so that the first technique can be applied again at later time.  The Esc stream processing system [69] comprises several components for task scheduling, performance monitoring, management of a resource pool to/from which machines are added/released, as well as applica- tion adaptation decisions. A processing element pro- cess executes UDFs and contains a manager and multi- ple workers, which serve respectively as a gateway for the element itself and for executing multiple instances of the UDF. The PE manager employs a function for balancing the load among workers. Each worker con- tains a buffer or queue and an operator. The autonomic manager of the system process monitors the load of ma- chines and the length of the worker processes. For adap- tation purposes, the autonomic manager can add/remove machines, replace the load balancing function of a PE manager and spawn/kill new workers, kill the PE man- ager and its workers altogether. The proposed elastic policies are based on load thresholds that, when ex- ceeded, trigger the execution of actions such as attach- ing new machines.  StreamCloud (SC) [93] provides multiple cloud par- allelisation techniques for splitting stream processing queries that it assigns to independent subclusters of computing resources. According to the chosen tech- nique, the number of resulting subqueries depends on the number of stateful operators that the original query contains. A subquery comprises a stateful operator and all intermediate stateless operators until another state- that receive output tuples from a subcluster. Bucket- Instance Maps (BIMs) control the distribution of buck- ets to downstream subclusters, which may be dynami- cally modified by Load Balancers (LBs). A load bal- ancer is an operator that distributes tuples from a sub- query to downstream subqueries. To manage elasticity, SC employs a resource management architecture that monitors CPU utilisation and, if the utilisation is out of pre-determined lower or upper thresholds, it can: ad- justs the system to rebalance the load; or provision or releases resources.  Heinze et al. [87] attempt to model the spikes in a query’s end-to-end latency when moving operators across machines, while trying to reduce the number of latency violations. Their target system, FUGU, consid- ers two classes of scaling decisions, namely mandatory, which are operator movements to avoid overload; and optional, such as releasing an unused host during light load. FUGU employs the Flux protocol for migrating stream processing operators [66]. Algorithms are pro- posed for scale out/in operations as well as operator placement. The scale-out solution extends the subset sum algorithm, where subsets of operators whose total load is below a pre-established threshold are considered to remain in a host. To pick a final set, the algorithm takes into consideration the latency spikes caused by moving the operators that are not in the set. For scale-in, FUGU releases a host with minimum latency spike. The operator placement is an incremental bin packing prob- lem, where bins are nodes with CPU capacity, and items are operators with CPU load as weight. Memory and network are second-level constraints that prevent plac- ing operators on overloaded hosts. A solution based on the FirstFit decreasing heuristic is provided.  Gedik et al. [94] tackle the challenge of auto- parallelising distributed stream processing engines in general while focusing on IBM Streams. As defined by Gedik et al. [94], “auto-parallelisation involves lo- cating regions in the application’s data flow graph that can be replicated at run-time to apply data partition- ing, in order to achieve scale.” Their work proposes an elastic auto-parallelisation approach that handles state- ful operators and general purpose applications. It also provides a control algorithm that uses metrics such as the blocking time at the splitter and throughput to de- termine how many parallel channels provide the best throughput. Data splitting for a parallel region can be performed in a round-robin manner if the region is state- less, or using a hash-based scheme otherwise.  Also considering IBM Streams. Tang and Gedik [37]  threads can level out the resource utilisation and im- prove throughput. They consider an execution model that comprises a set of threads, where each thread ex- ecutes a pipeline whose length extends from a start- ing operator port to a data sink or the port of another thread’s first operator. They use the notion of utility to model the goodness of including a new thread and pro- pose an optimisation algorithm find and evaluating par- allelisation options. Gedik et al. [36] propose a solution for IBM Streams exploiting pipeline parallelism and data parallelism simultaneously. They propose a tech- nique that segments a chain-like data flow graph into regions according to whether the operators they contain can be replicated or not. For the parallelisable regions, replicated pipelines are created preceded and followed by, respectively split and merge operators.  Wu and Tan [67] discuss technical challenges that may require a redesign of distributed stream process- ing systems, such as maintaining large amounts of state, workload fluctuation and multi-tenant resource sharing. They introduce ChronoStream, a system to support elas- ticity and high availability in latency-sensitive stream computing. To facilitate elasticity and operator migra- tion, ChronoStream divides the application-level state into a collection of computation slices that are periodi- cally check-pointed and replicated to multiple specified computing nodes using locality-sensitive techniques. In the case of component failure or workload redistribu- tion, it reconstructs and reschedules slice computation. Unlike D-Streams, ChronoStream provides techniques for tracking the progress of computation for each slice to reduce the overhead of reconstructing if information about the lineage graph is lost from memory.  STream processing ELAsticity (Stela) is a system ca- pable of optimising throughput after a scaling out/in operation and minimising the interruption to compu- tation while the operation is being performed [95]. It uses Expected Throughput Percentage (ETP), which is a per-operator performance metric defined as the “fi- nal throughput that would be affected if the operator’s processing speed were changed”. While evaluation re- sults demonstrate that ETP performs well as a post- scaling performance estimate, the work considers state- less operators whose migration can be performed with- out copying large amounts of application-related data. Stela is implemented as an extension to Storm’s sched- uler. Scale out/in operations are user-specified and are utilised to determine which operators are given more re- sources or which operators lose previously allocated re- sources. or decreases the number of processing operators to im- prove performance and resource utilisation. They intro- duce two algorithms to determine the state of an oper- ator, namely a short-term algorithm that evaluates load over short periods to detect traffic peaks; and (ii) a long- term algorithm that finds traffic patterns. The short- term algorithm compares the actual load of an operator against upper and lower thresholds. The long-term al- gorithm uses a Markov chain based on operator history to evaluate state transitions over the analysed samples to define the matrix transition. The algorithm estimates for the next time-window the probability that an operator reaches one of the three possible states (i.e. overloaded, underloaded, stable).  In the recent past, researchers and practitioners have also exploited the use of containers and lightweight resource virtualisation to perform migration of stream processing operators. Pahl and Lee [97] review con- tainer technology as means to tackle elasticity in highly distributed environments comprising edge and cloud computing resources. Both containers and virtualisa- tion technologies are useful when adjusting resource ca- pacity during scale out/in operations, but containers are more lightweight, portable and provide more agility and flexibility when testing and deploying applications.  To support operator placement and migration in Mobile Complex Event Processing (MCEP) systems, Ottenwalder et al. [98] present techniques that exploit system characteristics and predict mobility patterns for planning operator-state migration in advance. The en- visioned infrastructure comprises a federation of dis- tributed brokers whose hierarchy comprises a combina- tion of cloud and fog resources. Mobile nodes connect to the nearest broker, and each operator along with its state are kept in their own virtual machine. The problem tackled consists of finding a sequence of placements and migrations for an application graph so that the network utilisation is minimised and the end-to-end latency re- quirements are met. The system performs an incremen- tal placement where, a placement decision is enforced if its migration costs can be amortised by the gain of the next placement decision. A migration plan is dy- namically updated for each operator and a time-graph model is used for selecting migration targets and for ne- gotiating the plans with dependent operators to find the minimum cost plans for each operator and reserve re- sources accordingly. The link load created by events is estimated considering the most recent traffic measure- ments, while latency is computed via regular ping mes- sages or using Vivaldi coordinates [99].  details the infrastructure targeted by the solutions (i.e. cluster, cloud, fog); the types of operators considered (i.e. stateless, stateful); the metrics monitored and taken into account when planning a scale out/in operation; the type of elasticity envisioned (i.e. vertical or horizontal); and the elasticity actions performed during the execu- tion of a scale out/in operation.  5.3. Change and Burst Detection  Another approach that may be key to addressing elas- ticity in data stream processing is to use techniques to detect changes or bursts in the incoming data feeding a stream processing engine. This approach does not ad- dress elasticity per se, but it can be used with other tech- niques to trigger scale out/in operations such as adding or removing resources and employing graph adaptation.  For instance, Zhu and Shasha [100] introduce a shifted wavelet tree data structure for detecting bursts in aggregates of time series based data streams. They considered three types of sliding windows aggregates:  e Landmark windows: aggregates are computed from a specific time point.  e Sliding Windows: aggregates are calculated based on a window of the last n values.  e Damped window: the weights of data decrease ex- ponentially into the past.  Krishnamurthy et al. [90] propose a sketch data struc- ture for summarising network traffic at multiple levels on top of which time series forecast models are applied to detect significant changes in flows that present large forecast errors. Previous work provides a literature re- view on the topic of change and burst deception. Tran et al. [101], for instance, present a survey on change detection techniques for data stream processing.  6. Distributed and Hybrid Architecture  Most distributed data stream processing systems have been traditionally designed for cluster environments. More recently, architectural models have emerged for more distributed environments spanning multiple data centres or for exploiting the edges of the Internet (i.e., edge and fog computing [10, 102]).Existing work aims to use the Internet edges by trying to place certain stream processing elements on micro data centres (often called Cloudlets [1031]) closer to where the data is gen- so oe ee Se Sea ene  Target In-  Solution frastructure Operator type Metrics for Elasticity Elasticity Type Actions Femandez et al. cloud stateful Resource use (CPU) horizontal Operator state check-pointing, [86] fission T-Storm [91] cluster stateless : Resource use (CPU, N/A executor reassignment, inter-executor traffic load) topology rebalance Adaptive Storm stateful bolts, Resource use (CPU, inter-node executor placement, dynamic cluster stateless N/A traffic executor rea: 92 operators ffi Nephele SPE [89] cluster stateless Sy sanel Isteney) and vertical data batching, operator fission . - . replace load balancing Esc [69] cloud stateless! Resource use (machine load), horizontal functions dynamically, system metrics (queue lengths) operator fission StreamCloud (SC) SMSO tatetess and ; query splitting and placement, rivate Resource use (CPU) horizontal compiler for query [93] P stateful cloud? . parallelisation FUGU [87] cloud stateful Resource use (CPU, network horizontal operator migration, query and memory consumption) placement stateless and System metrics (congestion operator fission, state Gedik et al. [94] cluster partitioned ys index throu h D vertical check-pointing, operator stateful > en! migration . operator state check-pointing, ChronoStream [67] cloud stateful N/A vertical ane replication, migration, parallelism Stela [95] cloud stateless Se eushpat horizontal? operator fission and migration System metrics (load on event operator placement and MigCEP [98] cloud + fog stateful streams, inter-operator N/A P' P  latency)  migration  T Esc experiments consider only stateless operators. ? Nodes must be pre-configured with StreamCloud.  3 Execution of scale out/in operations are user-specified, not triggered by the system.  [105], or by exploiting mobile devices in the fog for stream processing [106]. Proposed architecture aims to place data analysis tasks at the edge of the Internet in order to reduce the amount of data transferred from sources to the cloud, improve the end-to-end latency, or offload certain analyses from the cloud [43].  Despite many efforts on building infrastructure, such as adapting OpenStack to run on cloudlets, much of the existing work on stream processing, however, remains at a conceptual or architectural level without concrete software solutions or demonstrated scalability. Appli- cations are still emerging. This section provides a non- exhaustive list of work regarding virtualisation infras- tructure for stream processing. and placement and re-  6.1. Lightweight Virtualisation and Containers  Pahl et al. [107] and Ismail et al. [108] discussed the use of lightweight virtualisation and the need for orchestrating the deployment of containers as key re- quirements to address challenges in infrastructure com- prising fog and cloud computing, such as improving ap- plication deployment speed, reducing overhead and data transferred over the network. Stream processing is of- ten viewed as a motivating scenario. Yangui et al. [109] propose a Platform as a Service (PaaS) architecture for cloud and fog integration. A proof-of-concept imple- mentation is described, which extends Cloud Foundry 110] to ease testing and deployment of applications whose components can be hosted either in the cloud or ‘O° resources.  advantage of lightweight containers to achieve resource elasticity. The solution exploits single board computers (e.g. Raspberry Pi 2 B, and Odroid C1+) as gateways where certain functional blocks (i.e. data compression and data processing) can be hosted. Similarly Petrolo et al. [113] focus on a gateway design for Wireless Sen- sor Network (WSN) to optimise the communication and make use of the edges. The gateway, designed for a cloud of things, can manage semantic-like things and work as an end-point for data presentation to users.  Hochreiner et al. [114] propose the Vlenna ecosys- tem for elastic Stream Processing (VISP) which exploits the use of lightweight containers to enable application deployment on hybrid environments (e.g. clouds and edge resources), a graphical interface for easy assem- ble of processing graphs, and reuse of processing op- erators. To achieve elasticity, the ecosystem runtime component monitors performance metrics of operators instances, the load on the message infrastructure, and introspection of the individual messages in the message queue.  6.2. Application Placement and Reconfiguration  Task scheduling considering hybrid scenarios has been investigated in other domains, such as mobile clouds [115] and heterogeneous memory [116]. For stream processing, Benoit ef al. [117] show that scheduling linear chains of processing operators onto a cluster of heterogeneous hardware is an NP-Hard prob- lem, whereas placement of virtual computing resources and network flows onto hybrid infrastructure has also been investigated in other contexts [118].  For stream processing, Cardellini et al. [119] in- troduce an integer programming formulation that takes into account resource heterogeneity for the Optimal Distributed Stream Processing Problem (ODP). They propose an extension to Apache Storm to incorporate an ODP-based scheduler, which estimates networks la- tency via a network coordination system built using the Vivaldi algorithm [99]. It has been shown, however, that assigning stream processing operators on VMs and placing them across multiple geographically distributed data centres while minimising the overall inter data- centre communication cost, can often be classified as an NP-Hard [120] problem or even NP-Complete [121]. Over time, however, cost-aware heuristics have been proposed for assigning stream processing operators to VMs placed across multiple data centres [120, 122].  Sajjad and Danniswara [42] introduce a stream pro- cessing solution. ie. SpnanEdge. that uses central and  worker is hosted at a central data centre and a spoke- worker at an edge data centre. SpanEdge also enables global and local tasks, and its scheduler attempts to place local tasks near the edges and global tasks at cen- tral data centres to minimise the impact of the latency of Wide Area Network (WAN) links interconnecting the data centres.  Mehdipour et al. [123] introduce a hierarchical ar- chitecture for processing streamlined data using fog and cloud resources. They focus on minimising communi- cation requirements between fog and cloud when pro- cessing data from IoTs devices. Shen et al. [124] ad- vocate the use of Cisco’s Connected Streaming Analyt- ics (CSA) for conceiving an architecture for handling data stream processing queries for loT applications by exploiting data centre and edge computing resources. Connected Streaming Analytics (CSA) provides a query language for continuous queries over streams.  Geelytics is a system tailored for loT environments that comprise multiple geographically distributed data producers, result consumers, and computing resources that can be hosted either on the cloud or at the network edges [125]. Geelytics follows a master-worker archi- tecture with a publish/subscribe service. Similarly to other data stream processing systems, Geelytics struc- tures applications as DAGs of operators. Unlike other systems, however, it enables scoped tasks, where a user specifies the scope granularity of each task comprising the processing graph. The scope granularity of tasks and data-consumer scoped subscriptions are used to de- vise the execution plan and deploy the resulting tasks according to the geographical location of data produc- ers.  7. Future Directions  Organisations often demand not only online process- ing of large amounts of streaming data, but also solu- tions that can perform computations on large data sets by using models such as MapReduce. As a result, big data processing solutions employed by large organisa- tions exploit hybrid execution models (e.g. using batch and online execution) that can span multiple data cen- tres. In addition to providing elasticity for computing and storage resources, ideally, a big data processing ser- vice should be able to allocate and release resources on demand. This section highlights some future directions.  7.1. SDN and In-Transit Processing Networks are becoming increasingly prosrammable Network (SDN) [126] and Network Functions Virtu- alization (NFV), which can provide mechanisms re- quired for allocating network capacity for certain data flows both within and across data centres with certain computing operations been performed in-network. In- transit stream processing can be carried out where cer- tain processing elements, or operators, are placed along the network interconnecting data sources and the cloud. This approach raises security and resource management challenges. In scenarios such as IoT, having compo- nents that perform processing along the path from data sources to the cloud can increase the number of hops susceptible to attacks. Managing task scheduling and al- location of heterogeneous resources whilst offering the elasticity with which cloud users are accustomed is also difficult as adapting an application to current resource and network conditions may require migrating elements of a data flow that often maintain state.  Most of the existing work on multi-operator place- ment considered network metrics such as latency and bandwidth while proposing decentralised algorithms, without taking into account that the network can be programmed and capacity allocated to certain network flows. The interplay between hybrid models and SDN as well as joint optimisation of application placement and flow routing can be better explored. The optimal placement of data processing elements and adaptation of data flow graphs, however, are hard problems.  In addition to placing operators on heterogeneous en- vironments, a key issue is deciding which operators are worth placing on edge computing resources and which should remain in the cloud. Emerging cognitive assis- tance scenarios [3] offer interesting use cases where ma- chine learning models can be trained on the cloud, and once trained they can be deployed on edge computing resources. The challenge, however, is to identify even- tual concept drifts that in turn require retraining a model and potentially adapting the execution data flow.  7.2. Programming Models for Hybrid and Highly Distributed Architecture  Frameworks that provide high-level programming ab- stractions have been introduced in recent past to ease the development and deployment of big data applica- tions that use hybrid models [8, 9]. Platform bindings have been provided to deploy applications developed using these abstractions on the infrastructure provided by commercial public cloud providers and open source solutions. Although such solutions are often restricted to a single cluster or data centre, efforts have been made to leverage resources from the edges of the Internet to  20  perform distributed queries [127] or to push frequently- performed analytics tasks to edge resources [125]. With the growing number of scenarios where data is collected by a plethora of devices, such as in loT and smart cities, and requires processing under low latency, solutions are increasingly exploiting resources available at the edges of the Internet (i.e. edge and fog computing). In addi- tion to providing means to place data processing tasks in such environments while minimising the use of net- work resources and latency, efficient methods to man- age resource elasticity in these scenarios should be in- vestigated. Moreover, high-level programming abstrac- tions and bindings to platforms capable of deploying and managing resources under such highly distributed scenarios are desirable.  Under the Apache Beam project [75], efforts have been made towards providing a unified SDK while enabling processing pipelines to be executed on dis- tributed processing back-ends such as Apache Spark [64] and Apache Flink [62]. Beam is particularly use- ful for embarrassingly parallel applications. There is still a lack of unified SDKs that simplify application development covering the whole spectrum, from data collection at the internet edges to processing at micro data centres (more closely located to the Internet edges) and data centres. Concerning resource management for such environments, several challenges arise regarding the network infrastructure and resource heterogeneity. Despite the challenges regarding state management for stream processing systems, container-based solutions could facilitate the deployment and elasticity manage- ment under such environments [128], and solutions such as Apache Quarks/Edgent [12] can be leveraged to per- form certain analyses at the Internet edges.  8. Summary and Conclusions  This paper discussed solutions for stream process- ing and techniques to manage resource elasticity. It first presented how stream processing fits in the over- all data processing framework often employed by large organisations. Then it presented a historical perspec- tive on stream processing engines, classifying them into three generations. After that, we elaborated on third- generation solutions and discussed existing work that aims to manage resource elasticity for stream process- ing engines. In addition to discussing the management of resource elasticity, we highlighted the challenges in- herent to adapting stream processing applications dy- namically in order to use additional resources made available during scale out operations, or release unused capacity when scaling in. The work then discussed emerging distributed architecture for stream processing and future directions on the topic. We advocate the need for high-level programming abstractions that enable de- velopers to program and deploy stream processing ap- plications on these emerging and highly distributed ar- chitecture more easily, while taking advantage of re- source elasticity and fault tolerance.  Acknowledgements  We thank Rodrigo Calheiros (Western Sydney Uni- versity), Srikumar Venugopal (IBM Research Ireland), Xunyun Liu (The University of Melbourne), and Pi- otr Borylo (AGH University) for their comments ona preliminary version of this work. This work has been carried out in the scope of a joint project between the French National Center for Scientific Research (CNRS) and the University of Melbourne.
"""